/*
 * Histogram Computation using CUDA
 *
 * This CUDA program computes a histogram from an array of integers.
 * It provides two implementations: a naive version and an optimized version.
 *
 * Naive Version:
 *   The naive implementation uses atomic operations to increment histogram bins
 *   based on the input array elements. Two CUDA kernels are employed: one for
 *   histogram computation and another for saturating the histogram to a predefined
 *   maximum value.
 *
 * Optimized Version:
 *   The optimized implementation reduces atomic collisions by having each thread
 *   block maintain its own local histogram in shared memory. This is achieved
 *   through two CUDA kernels: one for computing local histograms and another for
 *   accumulating and saturating them into a global histogram.
 *
 * Compilation:
 * $ nvcc histogram.cu -o histogram
 *
 * Execution:
 * $ ./histogram <input_size>
 *
 * Parameters:
 * <input_size> - Number of elements in the input array.
 *
 * Profiling with Nvidia Nsight:
 *   1. Compile the code with profiling information:
 *      $ nvcc -lineinfo histogram.cu -o histogram
 *
 *   2. Run the executable with Nvidia Nsight profiling:
 *      $ ncu -o histogram_profile -f ./histogram.exe <input_size>
 *
 *   3. Analyze the profiling results using Nvidia Nsight Compute:
 *      $ ncu-ui ./histogram_profile.ncu-rep
 *
 * Note: CUDA toolkit must be installed and configured for compilation.
 */

#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <random>
#include <cuda_runtime.h>

#define NUM_BINS 4096
#define THREADS_PER_BLOCK 512
#define HISTOGRAM_MAX_VALUE 127

// Uncomment the following line to compile and
// use the optimized version of the program.
#define OPTIMIZED_VERSION


//____________________________________________________________________________________________________________
// Timer Helpers

double start, stop;

void startTimer()
{
    start = (double)clock();
    start = start / CLOCKS_PER_SEC;
}

void stopTimer(const char* _message)
{
    stop = (double)clock();
    stop = stop / CLOCKS_PER_SEC;

    double elapsedTime = (stop - start) * 1.0e3;
    printf("%s: %.6f ms\n", _message, elapsedTime);
}


//____________________________________________________________________________________________________________
// CUDA Kernels

#if defined(OPTIMIZED_VERSION)
    /**
     * @brief CUDA kernel for an optimized histogram computation using shared memory.
     *
     * This kernel employs shared memory to reduce atomic collisions by having each thread block
     * maintain its own local histogram. Each thread processes a subset of elements from the input
     * array, incrementing the corresponding local bin in the shared memory using atomicAdd.
     * The local histograms are then copied to the global memory.
     *
     * @param _elements Pointer to the input array of elements.
     * @param _localBins Pointer to the output array of local histogram bins.
     * @param _numElements Total number of elements in the input array.
     */
    __global__ void optiHistogramKernel(unsigned int* _elements, unsigned int* _localBins, unsigned int _numElements)
    {
        int localThreadIdx = threadIdx.x;
        int localThreadCount = blockDim.x;
        int threadBlockIdx = blockIdx.x;
        int threadBlockCount = gridDim.x;

        int globalThreadIdx = localThreadCount * threadBlockIdx + localThreadIdx;
        int globalThreadCount = localThreadCount * threadBlockCount;

        // Out-of-bound early discard
        if (globalThreadIdx >= _numElements)
        {
            return;
        }

        // 1. Init shared local histogram to 0.
        __shared__ unsigned int sharedLocalBins[NUM_BINS];

        for (int i = localThreadIdx; i < NUM_BINS; i += localThreadCount)
        {
            sharedLocalBins[i] = 0;
        }

        __syncthreads();

        // 2. Compute local histogram for the subset of _elements assigned to current thread block.
        for (int i = globalThreadIdx; i < _numElements; i += globalThreadCount)
        {
            unsigned int binValue = _elements[i];
            atomicAdd(&sharedLocalBins[binValue], 1);
        }

        __syncthreads();

        // 3. Copy shared local histogram back to global memory.
        _localBins += threadBlockIdx * NUM_BINS;

        for (int i = localThreadIdx; i < NUM_BINS; i += localThreadCount)
        {
            _localBins[i] = sharedLocalBins[i];
        }
    }

    /**
     * @brief CUDA kernel to accumulate local histograms into the global one, and saturate the result.
     *
     * This kernel processes the local histograms generated by each thread block and accumulates
     * them into the global histogram. It also saturates the final histogram to a predefined maximum value.
     *
     * @param _localBins Pointer to the input array of local histogram bins.
     * @param _histogram Pointer to the output global histogram array.
     * @param _numLocalBins Number of local histograms (= grid size of the previous kernel).
     */
    __global__ void optiConvertKernel(unsigned int* _localBins, unsigned int* _histogram, unsigned int _numLocalBins)
    {
        int i = blockDim.x * blockIdx.x + threadIdx.x;

        // Out-of-bound early discard
        if (i >= NUM_BINS)
        {
            return;
        }

        // 1. Accumulate local histograms for the current bin.
        unsigned int binSum = 0;

        for (int j = 0; j < _numLocalBins; j++)
        {
            binSum += _localBins[i + j * NUM_BINS];
        }

        // 2. Saturate histogram to the predefined max value.
        _histogram[i] = min(binSum, HISTOGRAM_MAX_VALUE);
    }
#else
    /**
     * @brief CUDA kernel for a basic histogram computation.
     *
     * This is a naive implementation that assigns each thread to process a subset of elements
     * from the input array, incrementing the corresponding bin in the histogram using atomicAdd.
     *
     * @param _elements Pointer to the input array of elements.
     * @param _histogram Pointer to the output histogram array.
     * @param _numElements Total number of elements in the input array.
     */
    __global__ void naiveHistogramKernel(unsigned int* _elements, unsigned int* _histogram, unsigned int _numElements)
    {
        int globalThreadIdx = blockDim.x * blockIdx.x + threadIdx.x;
        int globalThreadCount = blockDim.x * gridDim.x;

        // The for loop ensures the array is fully processed, even if 
        // the number of allocated threads is inferior to _numElements.
        for (int i = globalThreadIdx; i < _numElements; i += globalThreadCount)
        {
            unsigned int binValue = _elements[i];
            atomicAdd(&_histogram[binValue], 1);
        }
    }

    /**
     * @brief CUDA kernel to saturate the previoulsy produced histogram.
     *
     * This is a basic implementation that assigns each thread to process a subset of bins
     * from the histogram, saturating their values to a predefined maximum.
     *
     * @param _histogram Pointer to the histogram array.
     */
    __global__ void naiveConvertKernel(unsigned int* _histogram)
    {
        int globalThreadIdx = blockDim.x * blockIdx.x + threadIdx.x;
        int globalThreadCount = blockDim.x * gridDim.x;

        // The for loop ensures all histogram bins are saturated, even if 
        // the number of allocated threads is inferior to NUM_BINS.
        for (int i = globalThreadIdx; i < NUM_BINS; i += globalThreadCount)
        {
            _histogram[i] = min(_histogram[i], HISTOGRAM_MAX_VALUE);
        }
    }
#endif


//____________________________________________________________________________________________________________
// Entry Point

int main(int _argc, char** _argv)
{
    if (_argc != 2)
    {
        fprintf(stderr, "Incorrect input, usage is: ./histogram.exe <input_size>\n");
        exit(EXIT_FAILURE);
    }

    // Retrieves array length from the cmd line.
    unsigned int arrayLength = atoi(_argv[1]);
    const int sizeofInput = arrayLength * sizeof(unsigned int);
    const int sizeofHistogram = NUM_BINS * sizeof(unsigned int);

    unsigned int* hostInput = (unsigned int*)malloc(sizeofInput);
    unsigned int* hostHistogram = (unsigned int*)malloc(sizeofHistogram);
    unsigned int* refHistogram = (unsigned int*)malloc(sizeofHistogram);

    // Initializes reference histogram with 0.
    memset(refHistogram, 0, sizeofHistogram);

    // Fills input array with random integers in range [0, NUM_BINS - 1], and
    // pre-compute histogram to use as a reference when validating GPU result.
    for (int i = 0; i < arrayLength; ++i)
    {
        unsigned int randValue = rand() % NUM_BINS;
        hostInput[i] = randValue;

        if (refHistogram[randValue] < HISTOGRAM_MAX_VALUE)
        {
            refHistogram[randValue]++;
        }
    }

    unsigned int* deviceInput;
    unsigned int* deviceHistogram;

    // Allocates GPU memory.
    cudaMalloc((void**)&deviceInput, sizeofInput);
    cudaMalloc((void**)&deviceHistogram, sizeofHistogram);

    // Copies array input to the GPU and initializes bins output to 0.
    cudaMemcpy(deviceInput, hostInput, sizeofInput, cudaMemcpyHostToDevice);
    cudaMemset(deviceHistogram, 0, sizeofHistogram);

    // Adapts 1D thread grid dimensions to the size of the input array.
    const int histogramBlockSize = THREADS_PER_BLOCK;
    const int histogramGridSize = (arrayLength + histogramBlockSize - 1) / histogramBlockSize;

    #if defined(OPTIMIZED_VERSION)
        // Allocates local histogram bins and fills them with 0.
        unsigned int* deviceLocalBins;
        const unsigned int numLocalBins = histogramGridSize;
        cudaMalloc((void**)&deviceLocalBins, NUM_BINS * numLocalBins);
        cudaMemset(deviceLocalBins, 0, NUM_BINS * numLocalBins);
    #endif

    // Profiling scope: histogram kernel
    startTimer();
    {
        #if defined(OPTIMIZED_VERSION)
            // Runs optimized version of the GPU kernel.
            optiHistogramKernel<<<histogramGridSize, histogramBlockSize>>>(deviceInput, deviceLocalBins, arrayLength);
        #else
            // Runs naive version of the GPU kernel.
            naiveHistogramKernel<<<histogramGridSize, histogramBlockSize>>>(deviceInput, deviceHistogram, arrayLength);
        #endif

        cudaDeviceSynchronize();
        cudaError_t cudaError = cudaGetLastError();

        if (cudaError != cudaSuccess)
        {
            fprintf(stderr, "CUDA error for histogram kernel: %s\n", cudaGetErrorString(cudaError));
        }
    }
    stopTimer("Histogram Kernel Time");

    // Adapts 1D thread grid dimensions to the number of bins.
    const int convertBlockSize = THREADS_PER_BLOCK;
    const int convertGridSize = (NUM_BINS + convertBlockSize - 1) / convertBlockSize;

    // Profiling scope: convert kernel
    startTimer();
    {
        #if defined(OPTIMIZED_VERSION)
            // Runs optimized version of the GPU kernel.
            optiConvertKernel<<<convertGridSize, convertBlockSize>>>(deviceLocalBins, deviceHistogram, numLocalBins);
        #else
            // Runs naive version of the GPU kernel.
            naiveConvertKernel<<<convertGridSize, convertBlockSize>>>(deviceHistogram);
        #endif

        cudaDeviceSynchronize();
        cudaError_t cudaError = cudaGetLastError();

        if (cudaError != cudaSuccess)
        {
            fprintf(stderr, "CUDA error for convert kernel: %s\n", cudaGetErrorString(cudaError));
        }
    }
    stopTimer("Convert Kernel Time");

    // Copies the GPU memory back to the CPU.
    cudaMemcpy(hostHistogram, deviceHistogram, sizeofHistogram, cudaMemcpyDeviceToHost);

    // Compares result with the reference.
    for (int i = 0; i < NUM_BINS; ++i)
    {
        if (hostHistogram[i] != refHistogram[i])
        {
            fprintf(stderr, "Result mismatch for integer %d: %u != %u\n", i, hostHistogram[i], refHistogram[i]);
            break;
        }
    }

    // Deallocates GPU memory.
    cudaFree(deviceInput);
    cudaFree(deviceHistogram);

    #if defined(OPTIMIZED_VERSION)
        cudaFree(deviceLocalBins);
    #endif

    // Deallocates CPU memory.
    free(hostInput);
    free(hostHistogram);
    free(refHistogram);

    return EXIT_SUCCESS;
}
